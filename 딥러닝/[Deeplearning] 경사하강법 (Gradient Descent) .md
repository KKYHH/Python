# [Deeplearning] 경사하강법 (Gradient Descent)

# 경사하강법 (Gradient Descent)

경사하강법(Gradient Descent)은 딥러닝에서 가장 기본적인 최적화 방법 중 하나입니다. 

이 방법은 **손실 함수(Loss Function)**을 최소화하는 **파라미터(Weight)**를 찾기 위한 방법으로, **학습률(learning rate)**과 **반복 횟수(iteration)**를 조절하여 최적의 파라미터를 찾아냅니다.

경사하강법은 크게 두 가지 방법으로 나뉩니다. 

첫 번째는 Batch Gradient Descent 방법으로, 전체 데이터셋을 한번에 학습시키는 방법입니다. 

이 방법은 속도가 느리지만 정확한 결과를 얻을 수 있습니다. 

두 번째는 Stochastic Gradient Descent 방법으로, 데이터셋 중에서 랜덤으로 하나의 데이터만 선택하여 학습시키는 방법입니다. 이 방법은 빠른 학습이 가능하지만 정확도는 조금 떨어질 수 있습니다.

이 외에도 Mini-Batch Gradient Descent 방법, Momentum 방법, AdaGrad 방법, Adam 방법 등 다양한 경사하강법 알고리즘이 존재합니다. **이 중에서도 최근에는 Adam 방법이 가장 많이 사용되고 있습니다.**

경사하강법은 딥러닝에서 뿐만 아니라, 기계학습에도 널리 사용되는 방법으로, 이를 이해하고 구현할 수 있는 능력은 딥러닝 엔지니어로서 필수적인 역량입니다.

경사하강법을 이해하기 위해서는 미분 개념을 이해하는 것이 중요합니다. 미분은 함수의 변화율을 나타내는 것으로, **경사하강법에서는 손실 함수의 기울기(Gradient)를 계산하여 이를 이용해 파라미터를 업데이트합니다.**

하지만 경사하강법도 단점이 있습니다. 

학습률을 잘못 설정하면 발산하는 경우도 있으며, **지역 최소값(local minima)에 빠져 전역 최소값(global minima)을 찾지 못하는 문제도 있습니다.** 

이러한 문제를 해결하기 위해서는 **학습률**을 조절하는 방법이나, 초기값을 다양하게 설정하는 방법 등이 있습니다.

**경사하강법은 딥러닝에서 최적화 문제를 해결하는 데 있어서 가장 기본적이면서도 중요한 방법 중 하나입니다. 이를 잘 이해하고 구현할 수 있는 능력은 딥러닝 엔지니어로서 필수적인 역량입니다.**